# The bottleneck architecture supports three learning framework (i.e., losses)
# via model.model_type:
#   1) nll - Conditional cross entropy (the usual NMT loss)
#   2) mim - MIM learning framework. A latent variable model with good
#                   reconstruction and compressed latent representation.
#                   https://arxiv.org/pdf/2003.02645.pdf
#   3) vae - VAE learning framework. A latent variable model which learns
#                   good probability estimation over observations and
#                   a regularized latent representation.
#                   https://arxiv.org/pdf/1312.6114.pdf
# The bottleneck architecture supports three encoder architectures via
# model.encoder.arch:
#   1) seq2seq -  the usual NMT model without bottleneck
#   2) bridge - a bottleneck which projects the encoder output to a fixed
#               number of steps using attention bridge (https://arxiv.org/pdf/1703.03130.pdf)
#   3) perceiver - a bottleneck by projecting inputs to a fixed
#               number of steps using perceiver architecture (https://arxiv.org/pdf/2103.03206.pdf)
#   4) max_pool / avg_pool - a reduction by halving the number of steps at the end of every hidden block.
#                            reduction is using max pooling or average pooling.

defaults:
  - aayn_base

name: AttentionIsAllYouNeedBottleneck

do_training: True # set to False if only preprocessing data
do_testing: False # set to True to run evaluation on test data after training

model:
  model_type: 'nll' # learning (i.e., loss) type: nll (i.e., cross-entropy/auto-encoder), mim, vae (see description above)
  min_logv: -6 # minimal allowed log variance for mim
  latent_size: -1 # dimension of latent (projected from hidden) -1 will take value of hidden size
  non_recon_warmup_batches: 200000 # warm-up steps for mim, and vae losses
  recon_per_token: true # when false reconstruction is computed per sample, not per token

  encoder:
    mask_future: false
    arch: perceiver # avg_pool, max_pool, seq2seq, bridge, perceiver (see description above)
    hidden_steps: 32 # fixed number of hidden steps
    hidden_blocks: 1 # number of repeat blocks (see classes for description)
    hidden_init_method: default # see classes for available values

  decoder:
    arch: seq2seq # currently only seq2seq is supported

exp_manager:
  name: AAYNBottleneck
