# Question Answering with SQUAD
name: &name QA

pretrained_model: null # pretrained QAModel model from list_available_models()
do_training: true # training mode, for testing change to false
trainer:
  devices: 1 # the number of gpus, 0 for CPU, or list with gpu indices
  num_nodes: 1
  max_epochs: 2 # the number of training epochs
  max_steps: -1 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  precision: 16 # 16 to use AMP
  accelerator: gpu
  strategy: ddp
  gradient_clip_val: 0.0
  val_check_interval: 1.0 # check once per epoch .25 for 4 times per epoch
  enable_checkpointing: False # provided by exp_manager
  logger: false # provided by exp_manager
  num_sanity_val_steps: 0
  log_every_n_steps: 1  # Interval of logging.

model:
  nemo_path: null # exported .nemo path
  dataset:
    version_2_with_negative: false
    # If true, the examples contain some that do not have an answer.
    doc_stride: 128
    # When splitting up a long document into chunks,
    # how much stride to take between chunks.
    max_query_length: 64
    # The maximum number of tokens for the question.
    # Questions longer than this will be truncated to
    # this length.
    max_seq_length: 384
    # The maximum total input sequence length after
    # WordPiece tokenization. Sequences longer than this
    # will be truncated, and sequences shorter than this
    # will be padded.
    max_answer_length: 30
    # The maximum length of an answer that can be
    # generated. This is needed because the start
    # and end predictions are not conditioned
    # on one another.
    null_score_diff_threshold: 0.0
    # If null_score - best_non_null is greater than the threshold predict null.
    n_best_size: 20
    # The total number of n-best predictions to generate at testing.
    use_cache: true
    do_lower_case: true
    # if true does lower case
    keep_doc_spans: all
    # if all, keep all doc spans
    # if only_positive, keep doc spans containing answer only
    # if limited_negative, keep 10 doc spans closest to answer per question

    num_workers:  2
    pin_memory: false
    drop_last: false

  train_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: true
    num_samples: -1
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  validation_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: false
    num_samples: -1    
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  test_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: false
    num_samples: -1
    # Default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}
    
  tokenizer:
    tokenizer_name: ${model.language_model.pretrained_model_name} # tokenizer that inherits from TokenizerSpec
    vocab_file: null # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null # expand the following to a dictionary if special tokens need to be added.
    #  only necessary for adding transformer/bert-specific special tokens to tokenizer if the tokenizer does not already have these inherently.

  language_model:
    pretrained_model_name: bert-base-uncased # BERT-like model name
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null # if specified initializes model from scratch

  token_classifier:
    num_layers: 1
    dropout: 0.0
    num_classes: 2
    activation: relu
    log_softmax: false
    use_transformer_init: true


  optim:
    name: adamw
    lr: 3e-5
    weight_decay: 0.0
    sched:
      name: SquareRootAnnealing

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

      # scheduler config override
      warmup_steps: null
      warmup_ratio: 0.0
      last_epoch: -1

exp_manager:
  exp_dir: null # where to store logs and checkpoints
  name: *name # name of experiment
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  create_wandb_logger: False
  wandb_logger_kwargs:
    name: ???
    project: QA

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null